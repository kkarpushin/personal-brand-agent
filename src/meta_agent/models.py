"""
Data models for the Meta-Agent subsystem.

Defines all dataclasses used by the self-modification, code evolution,
research, reflection, experimentation, autonomy, and knowledge base
components:

    - ApprovalTimeoutConfig: Configuration for approval timeout handling
    - AutonomyConfig: Configuration for autonomy management
    - AutonomyLevel: Autonomy levels (IntEnum, 1-4)
    - CapabilityGap: Identified missing capability
    - CapabilityType: Types of capabilities the system can have or lack
    - ClaudeCodeResult: Result from a Claude Code CLI execution
    - CONFIG_PATH_MAPPING / get_config_path: Config path resolution
    - DialogueSummary: Summary of a critique dialogue
    - EvaluationCriterion / evaluation_rubric: QC evaluation rubric
    - Experiment: A/B experiment definition and results
    - ExperimentStatus: Lifecycle status of an experiment
    - ExperimentVariant: A single variant in an A/B experiment
    - GeneratedCode: Result of code generation (with is_valid())
    - GeneratedModule: A Python module generated by the code evolution engine
    - ImprovementResult: Result of a full deep improvement loop
    - Learning: A piece of knowledge learned by the agent
    - ModificationRecord: Record of a self-modification
    - ModificationRequest: Request to modify system behavior
    - ModificationRiskLevel / modification_risk_classification: Risk safety
    - PendingApproval: Represents a pending approval request
    - PromptEvolution: Versioned evolution of a system prompt
    - Reflection: Result of self-reflection on critique and performance
    - RegisteredModule: A module registered in the system
    - ResearchFinding: A single insight discovered during research
    - ResearchQuery: A targeted research question for knowledge gap filling
    - ResearchRecommendation: An actionable recommendation from research
    - ResearchReport: Complete research report with findings and recommendations
    - ResearchTrigger: Enum of events that trigger a research cycle
    - RollbackResult: Result of a rollback operation
    - RollbackTrigger: Conditions that trigger automatic rollback
    - SelfModificationResult: Result of a self-modification attempt
    - SingleCallEvaluation: Complete evaluation from single LLM call
    - SystemSnapshot: Snapshot of system state before modification
    - VisualEvaluation: Result of visual quality evaluation

Architecture references:
    - ``architecture.md`` lines 14305-15480  (Meta-Agent overview)
    - ``architecture.md`` lines 15539-15587  (CapabilityType, CapabilityGap)
    - ``architecture.md`` lines 15718-15754  (GeneratedCode)
    - ``architecture.md`` lines 16410-16424  (RegisteredModule)
    - ``architecture.md`` lines 16657-16663  (SelfModificationResult)
    - ``architecture.md`` lines 17045-17117  (Research models)
    - ``architecture.md`` lines 18019-18054  (SystemSnapshot, RollbackResult)
    - ``architecture.md`` lines 18589-18597  (DialogueSummary)
    - ``architecture.md`` lines 18661-18684  (Reflection)
    - ``architecture.md`` lines 18764-19029  (ClaudeCodeResult)
    - ``architecture.md`` lines 19499-19519  (GeneratedModule, PromptEvolution)
    - ``architecture.md`` lines 19641-19652  (Learning)
    - ``architecture.md`` lines 19767-19774  (ImprovementResult)
    - ``architecture.md`` lines 19788-19804  (ModificationRecord)
    - ``architecture.md`` lines 19988-20032  (ExperimentStatus, ExperimentVariant, Experiment)
    - ``architecture.md`` lines 20622-20666  (AutonomyLevel, AutonomyConfig)
    - ``architecture.md`` lines 20956-21007  (ApprovalTimeoutConfig, PendingApproval)
    - ``architecture.md`` lines 22438-22895  (Modification Safety models)
    - ``architecture.md`` lines 22937-23466  (Single-Call Evaluation models)
"""

from __future__ import annotations

from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum, IntEnum
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from src.utils import utc_now, generate_id


# =============================================================================
# CAPABILITY TYPE (Enum)
# Architecture reference: architecture.md lines 15560-15568
# =============================================================================


class CapabilityType(Enum):
    """Types of capabilities the system can have or lack.

    Used by the CapabilityAnalyzer to classify missing capabilities and
    by the CodeGenerationEngine to determine the appropriate code to generate.

    Attributes:
        DATA_SOURCE: New data source integration.
        ANALYSIS_METHOD: New way to analyze content.
        GENERATION_STYLE: New content generation approach.
        VISUAL_FORMAT: New visual format support.
        INTEGRATION: External service integration.
        UTILITY: Helper functions/utilities.
        AGENT: Entirely new agent.
        VALIDATOR: New validation logic.
    """

    DATA_SOURCE = "data_source"
    ANALYSIS_METHOD = "analysis_method"
    GENERATION_STYLE = "generation_style"
    VISUAL_FORMAT = "visual_format"
    INTEGRATION = "integration"
    UTILITY = "utility"
    AGENT = "agent"
    VALIDATOR = "validator"


# =============================================================================
# RESEARCH TRIGGER (Enum)
# =============================================================================


class ResearchTrigger(Enum):
    """Events that trigger a research cycle.

    Continuous learning triggers fire frequently (every iteration or on
    component feedback), while scheduled/reactive triggers fire periodically
    or in response to performance signals.

    Architecture reference: ``architecture.md`` lines 17045-17061.

    Attributes:
        EVERY_ITERATION: After every evaluation cycle.
        FIRST_POST: Special bootstrap handling for the very first post.
        COMPONENT_FEEDBACK: When a specific component receives negative feedback.
        UNDERPERFORMANCE: 3+ recent posts score below average.
        NEW_CONTENT_TYPE: No historical experience with this content type.
        WEEKLY_CYCLE: Scheduled weekly deep research (Sunday).
        ALGORITHM_CHANGE: Detected engagement pattern shift.
        MANUAL_REQUEST: Human-initiated research request.
    """

    # Continuous learning triggers
    EVERY_ITERATION = "every_iteration"
    FIRST_POST = "first_post"
    COMPONENT_FEEDBACK = "component_feedback"

    # Scheduled / reactive triggers
    UNDERPERFORMANCE = "underperformance"
    NEW_CONTENT_TYPE = "new_content_type"
    WEEKLY_CYCLE = "weekly_cycle"
    ALGORITHM_CHANGE = "algorithm_change"
    MANUAL_REQUEST = "manual_request"


# =============================================================================
# MODIFICATION RISK LEVEL (Enum)
# Architecture reference: architecture.md lines 22438-22480
# =============================================================================


class ModificationRiskLevel(Enum):
    """Risk classification for system modifications.

    Determines the approval flow:
    - LOW: Auto-apply, monitor for 5 posts
    - MEDIUM: Auto-apply with stricter rollback, notify via Telegram
    - HIGH: Requires human approval before apply
    - CRITICAL: Requires human approval with explicit confirmation
    """

    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


# =============================================================================
# EXPERIMENT STATUS (Enum)
# Architecture reference: architecture.md lines 19988-19993
# =============================================================================


class ExperimentStatus(Enum):
    """Lifecycle status of an A/B experiment.

    Attributes:
        DRAFT: Experiment is designed but not yet started.
        RUNNING: Experiment is actively collecting data.
        COMPLETED: Experiment has enough data and results are final.
        STOPPED_EARLY: Experiment was stopped before completion.
        WINNER_APPLIED: Winning variant has been applied to production.
    """

    DRAFT = "draft"
    RUNNING = "running"
    COMPLETED = "completed"
    STOPPED_EARLY = "stopped_early"
    WINNER_APPLIED = "winner_applied"


# =============================================================================
# AUTONOMY LEVEL (IntEnum)
# Architecture reference: architecture.md lines 20622-20635
# =============================================================================


class AutonomyLevel(IntEnum):
    """Autonomy levels for the agent system.

    Supports numeric comparison (e.g. ``level >= AutonomyLevel.AUTO_HIGH_SCORE``).

    Level 1: Human approves everything (posts, modifications, research).
    Level 2: Human approves posts only, auto-modifications allowed.
    Level 3: Auto-publish high-score posts (>= 9.0), human for rest.
    Level 4: Full autonomy (human notified, not asked).
    """

    HUMAN_ALL = 1
    HUMAN_POSTS = 2
    AUTO_HIGH_SCORE = 3
    FULL_AUTONOMY = 4


# =============================================================================
# CAPABILITY GAP
# Architecture reference: architecture.md lines 15572-15586
# =============================================================================


@dataclass
class CapabilityGap:
    """Identified missing capability.

    Produced by the CapabilityAnalyzer when it detects that the system
    lacks a capability needed to produce better content.

    Attributes:
        id: Unique identifier for this gap.
        gap_type: Type of capability that is missing.
        description: What capability is missing.
        evidence: List of evidence strings explaining why we believe
            the capability is missing.
        proposed_solution: High-level solution description.
        priority: Priority level (1 = critical, 5 = nice-to-have).
        detected_at: UTC timestamp when the gap was detected.
        related_modules: Existing modules related to this gap type.
        required_interfaces: Interfaces the new capability must implement.
        example_usage: Example of how the new capability would be used.
    """

    id: str
    gap_type: CapabilityType
    description: str
    evidence: List[str]
    proposed_solution: str
    priority: int  # 1 = critical, 5 = nice-to-have
    detected_at: datetime = field(default_factory=utc_now)

    # Context for code generation
    related_modules: List[str] = field(default_factory=list)
    required_interfaces: List[str] = field(default_factory=list)
    example_usage: Optional[str] = None


# =============================================================================
# GENERATED CODE
# Architecture reference: architecture.md lines 15718-15754
# =============================================================================


@dataclass
class GeneratedCode:
    """Result of code generation by the CodeGenerationEngine.

    Contains the generated source code, metadata about what gap it addresses,
    and validation results from the code validator pipeline.

    Attributes:
        module_name: Name of the generated module.
        file_path: Filesystem path where the module will be saved.
        code: Complete Python source code.
        description: Human-readable description of what the code does.
        gap_id: ID of the ``CapabilityGap`` this code addresses.
        generated_at: UTC timestamp of generation.
        syntax_valid: Whether the code passed syntax (AST) validation.
        type_check_passed: Whether the code passed type checking (informational).
        tests_passed: Whether the generated tests pass.
        security_passed: Whether the code passed security scanning.
        test_code: Generated pytest test code, if any.
        dependencies: List of pip packages required by this code.
    """

    module_name: str
    file_path: str
    code: str
    description: str

    # Metadata
    gap_id: str
    generated_at: datetime = field(default_factory=utc_now)

    # Validation results (filled after validation)
    syntax_valid: bool = False
    type_check_passed: bool = False
    tests_passed: bool = False
    security_passed: bool = False

    # Generated tests
    test_code: Optional[str] = None

    dependencies: List[str] = field(default_factory=list)

    def is_valid(self) -> bool:
        """Check if generated code is valid for loading.

        ``type_check_passed`` is informational only (non-blocking).
        Type errors are logged as warnings but do not prevent loading.
        This allows code with minor type inconsistencies to still work.

        Returns:
            ``True`` if syntax_valid, tests_passed, and security_passed
            are all ``True``.
        """
        return all([
            self.syntax_valid,
            # type_check_passed is informational only - not required
            self.tests_passed,
            self.security_passed,
        ])


# =============================================================================
# REGISTERED MODULE
# Architecture reference: architecture.md lines 16410-16424
# =============================================================================


@dataclass
class RegisteredModule:
    """A module registered in the system's ModuleRegistry.

    Represents a loaded Python module that was either auto-generated
    or manually registered. Tracks version, capability type, and
    a runtime reference to the loaded module object.

    Attributes:
        name: Module name (valid Python identifier).
        path: Filesystem path to the module file.
        capability_type: What type of capability this module provides.
        loaded_at: UTC timestamp when the module was last loaded.
        version: Version counter, incremented on each hot-reload.
        module_ref: Runtime reference to the loaded module object.
        description: Human-readable description of what the module does.
        exports: List of public function/class names exported.
    """

    name: str
    path: Path
    capability_type: CapabilityType
    loaded_at: datetime
    version: int = 1

    # Runtime reference
    module_ref: Optional[Any] = None

    # Metadata
    description: str = ""
    exports: List[str] = field(default_factory=list)


# =============================================================================
# SELF-MODIFICATION RESULT
# Architecture reference: architecture.md lines 16657-16663
# =============================================================================


@dataclass
class SelfModificationResult:
    """Result of a self-modification attempt.

    Returned by the SelfModificationEngine after attempting to fill
    a capability gap through code generation.

    Attributes:
        success: Whether the modification succeeded.
        gap: The capability gap that was being addressed.
        generated_code: The generated code, if code generation was attempted.
        error: Error message if the modification failed.
        retry_with_new_capability: Whether to retry content generation
            with the newly loaded capability.
    """

    success: bool
    gap: CapabilityGap
    generated_code: Optional[GeneratedCode] = None
    error: Optional[str] = None
    retry_with_new_capability: bool = False


# =============================================================================
# CLAUDE CODE CLI RESULT
# =============================================================================


@dataclass
class ClaudeCodeResult:
    """Result from a Claude Code headless execution.

    Captures success/failure status, output text, session metadata,
    cost information, and lists of files created or modified during
    the execution.

    Attributes:
        success: Whether the execution completed successfully.
        result: The text output produced by Claude Code.
        session_id: Unique session identifier from the Claude Code run.
        cost_usd: Total cost in USD for the execution.
        duration_ms: Wall-clock duration in milliseconds.
        files_created: Paths of files created during execution.
        files_modified: Paths of files modified during execution.
        error: Error message if execution failed, ``None`` otherwise.
    """

    success: bool
    result: str
    session_id: str
    cost_usd: float
    duration_ms: int
    files_created: List[str]
    files_modified: List[str]
    error: Optional[str] = None


# =============================================================================
# RESEARCH QUERY
# =============================================================================


@dataclass
class ResearchQuery:
    """A targeted research question for filling a knowledge gap.

    Used by both the ResearchAgent (for deep research) and the Reflection
    engine (to specify what additional information the system needs).

    Architecture reference: ``architecture.md`` lines 17064-17071.

    Attributes:
        source: Data source to query (``"perplexity"``, ``"competitor_scrape"``,
            ``"own_data"``).
        query: The actual query string or action description.
        purpose: Why this research is being conducted.
        priority: Priority level (1 = highest, 5 = lowest).
    """

    source: str = "perplexity"
    query: str = ""
    purpose: str = ""
    priority: int = 1


# =============================================================================
# RESEARCH FINDING
# =============================================================================


@dataclass
class ResearchFinding:
    """A single insight discovered during research.

    Produced by the ResearchAgent when processing search results,
    competitor posts, or own performance data.

    Architecture reference: ``architecture.md`` lines 17073-17081.

    Attributes:
        finding: What was discovered.
        source: Where the insight came from (URL, competitor name, etc.).
        confidence: Confidence score (0.0-1.0) in the finding.
        actionable: Whether the finding can be directly acted upon.
        suggested_change: What to change based on this finding, if any.
        affected_component: Which pipeline component should change
            (``"writer"``, ``"trend_scout"``, ``"visual_creator"``, etc.).
    """

    finding: str
    source: str
    confidence: float
    actionable: bool
    suggested_change: Optional[str] = None
    affected_component: str = "writer"


# =============================================================================
# RESEARCH RECOMMENDATION
# =============================================================================


@dataclass
class ResearchRecommendation:
    """An actionable recommendation synthesized from research findings.

    Generated by the ResearchAgent after analyzing all findings from a
    research cycle.  Each recommendation targets a specific pipeline
    component with a concrete change.

    Architecture reference: ``architecture.md`` lines 17084-17093.

    Attributes:
        component: Target component (``"writer"``, ``"trend_scout"``, etc.).
        change: Description of what should be changed.
        priority: Priority level (1 = highest, 5 = lowest).
        rationale: Explanation of why this change is recommended.
        confidence: Confidence score (0.0-1.0) in the recommendation.
        estimated_impact: Expected impact level (``"high"``, ``"medium"``,
            ``"low"``).
        source_findings: References to supporting findings.
    """

    component: str
    change: str
    priority: int
    rationale: str
    confidence: float
    estimated_impact: str = "medium"
    source_findings: List[str] = field(default_factory=list)


# =============================================================================
# RESEARCH REPORT
# =============================================================================


@dataclass
class ResearchReport:
    """Complete research report with findings and recommendations.

    Produced at the end of a research cycle.  Contains the trigger, all
    queries executed, raw findings, synthesized recommendations, and
    confidence metadata.

    Architecture reference: ``architecture.md`` lines 17096-17117.

    Attributes:
        trigger: The ``ResearchTrigger`` that initiated this cycle.
        trigger_reason: Human-readable explanation of why research ran.
        started_at: UTC timestamp when research began.
        completed_at: UTC timestamp when research finished.
        queries_executed: List of research queries that were executed.
        findings: All insights discovered during research.
        recommendations: Actionable recommendations derived from findings.
        total_sources_consulted: Number of distinct sources queried.
        confidence_score: Overall confidence in the recommendations (0.0-1.0).
    """

    trigger: ResearchTrigger
    trigger_reason: str
    started_at: datetime
    completed_at: datetime
    queries_executed: List[ResearchQuery] = field(default_factory=list)
    findings: List[ResearchFinding] = field(default_factory=list)
    recommendations: List[ResearchRecommendation] = field(default_factory=list)
    total_sources_consulted: int = 0
    confidence_score: float = 0.0


# =============================================================================
# SYSTEM SNAPSHOT
# Architecture reference: architecture.md lines 18019-18044
# =============================================================================


@dataclass
class SystemSnapshot:
    """Snapshot of system state before a modification.

    Used for rollback if modification causes degradation.  Captures
    file contents (prompts, configs, generated modules) and optionally
    Supabase table rows affected by self-modification.

    Attributes:
        id: Unique identifier for this snapshot.
        created_at: UTC timestamp when the snapshot was taken.
        trigger: What triggered the change (research_id, etc.).
        description: Human-readable description of the change.
        prompts: Mapping of prompt file paths to their contents before change.
        configs: Mapping of config file paths to parsed JSON before change.
        generated_modules: List of generated module file paths before change.
        database_state: Snapshots of Supabase tables affected by the change.
            Format: ``{"learnings": [...rows...], "code_modifications": [...rows...]}``.
        performance_baseline: Metrics values before change.
        change_type: Type of change (``"prompt"``, ``"config"``, ``"code"``,
            ``"knowledge"``).
    """

    id: str
    created_at: datetime
    trigger: str
    description: str

    # What was changed (FILES)
    prompts: Dict[str, str]  # path -> content (before change)
    configs: Dict[str, Any]  # path -> parsed json (before change)
    generated_modules: List[str]  # list of file paths (before change)

    # Snapshots of Supabase tables affected by self-modification
    database_state: Dict[str, List[Dict[str, Any]]] = field(default_factory=dict)

    # Metadata
    performance_baseline: Dict[str, float] = field(default_factory=dict)
    change_type: str = "config"  # "prompt" / "config" / "code" / "knowledge"


# =============================================================================
# ROLLBACK RESULT
# Architecture reference: architecture.md lines 18047-18053
# =============================================================================


@dataclass
class RollbackResult:
    """Result of a rollback operation.

    Attributes:
        success: Whether the rollback completed successfully.
        snapshot_id: ID of the snapshot that was restored.
        files_restored: List of file paths that were restored.
        error: Error message if the rollback failed, ``None`` otherwise.
    """

    success: bool
    snapshot_id: str
    files_restored: List[str]
    error: Optional[str] = None


# =============================================================================
# DIALOGUE SUMMARY
# =============================================================================


@dataclass
class DialogueSummary:
    """Summary of a critique dialogue between agents.

    Produced by the CriticAgent (or SingleCallEvaluator acting as critic)
    at the end of a review session.  Used as input to the ReflectionEngine.

    Architecture reference: ``architecture.md`` lines 18589-18597.

    Attributes:
        weaknesses: Identified weaknesses in the evaluated content.
        suggestions: Specific improvement suggestions.
        knowledge_gaps: Areas where the agent lacks knowledge.
        research_queries: Suggested research queries to fill gaps.
        confidence_in_suggestions: Overall confidence (0.0-1.0) in the
            provided suggestions.
    """

    weaknesses: List[str] = field(default_factory=list)
    suggestions: List[str] = field(default_factory=list)
    knowledge_gaps: List[str] = field(default_factory=list)
    research_queries: List[str] = field(default_factory=list)
    confidence_in_suggestions: float = 0.5


# =============================================================================
# REFLECTION
# =============================================================================


@dataclass
class Reflection:
    """Result of self-reflection on critique and performance.

    Produced by the ReflectionEngine after analyzing feedback.
    Drives decisions about prompt changes, code changes, and
    process improvements.

    Architecture reference: ``architecture.md`` lines 18661-18684.

    Attributes:
        critique_valid: Whether the received critique is considered valid.
        critique_validity_reasoning: Explanation of why critique is/isn't valid.
        is_recurring_pattern: Whether this issue appears across multiple posts.
        pattern_description: Description of the recurring pattern, if any.
        pattern_frequency: How often the pattern occurs (e.g. "3 of last 10").
        knowledge_gaps: List of identified knowledge gaps.
        research_needed: Specific research queries to fill gaps.
        process_changes: Recommended changes to the content process.
        prompt_changes: Specific prompt modifications to make.
        code_changes: Code-level changes needed (new modules, functions).
        confidence_in_changes: Confidence score (0.0-1.0) for proposed changes.
    """

    # Validation of critique
    critique_valid: bool
    critique_validity_reasoning: str

    # Pattern detection
    is_recurring_pattern: bool
    pattern_description: Optional[str] = None
    pattern_frequency: Optional[str] = None  # "3 out of last 10 posts"

    # Knowledge gaps
    knowledge_gaps: List[str] = field(default_factory=list)
    research_needed: List[ResearchQuery] = field(default_factory=list)

    # Action items
    process_changes: List[str] = field(default_factory=list)
    prompt_changes: List[str] = field(default_factory=list)
    code_changes: List[str] = field(default_factory=list)

    # Confidence
    confidence_in_changes: float = 0.5


# =============================================================================
# MODIFICATION RECORD
# Architecture reference: architecture.md lines 19788-19804
# =============================================================================


@dataclass
class ModificationRecord:
    """Record of a self-modification.

    Tracks what was changed, why, and whether it can be rolled back.
    Stored in Supabase ``code_modifications`` table.

    Attributes:
        timestamp: UTC timestamp when the modification was applied.
        component: What component was modified (e.g. ``"writer"``).
        parameter: Specific parameter that was changed.
        old_value: Value before the modification.
        new_value: Value after the modification.
        reason: Human-readable explanation of why this change was made.
        research_report_id: Link to the research report that triggered this.
        rollback_available: Whether this modification can be rolled back.
        rolled_back: Whether this modification has been rolled back.
        rollback_reason: Explanation of why it was rolled back, if applicable.
    """

    timestamp: datetime
    component: str
    parameter: str
    old_value: Any
    new_value: Any
    reason: str
    research_report_id: str

    # For rollback
    rollback_available: bool = True
    rolled_back: bool = False
    rollback_reason: Optional[str] = None


# =============================================================================
# GENERATED MODULE
# =============================================================================


@dataclass
class GeneratedModule:
    """A Python module generated by the code evolution engine.

    Represents a complete, validated Python module that was auto-generated
    to fill a capability gap or implement a learning.

    Architecture reference: ``architecture.md`` lines 19499-19509.

    Attributes:
        name: Module name (valid Python identifier).
        path: Filesystem path where the module is saved.
        code: Complete Python source code of the module.
        purpose: Human-readable description of what the module does.
        generated_at: UTC timestamp of generation.
        knowledge_source: The knowledge/research that informed generation.
        validated: Whether the module passed syntax validation.
    """

    name: str
    path: Path
    code: str
    purpose: str
    generated_at: datetime = field(default_factory=utc_now)
    knowledge_source: Dict[str, Any] = field(default_factory=dict)
    validated: bool = False


# =============================================================================
# PROMPT EVOLUTION
# =============================================================================


@dataclass
class PromptEvolution:
    """Versioned evolution of a system prompt.

    Captures the before/after state of a prompt evolution, including
    the reasoning and knowledge that drove the changes.

    Architecture reference: ``architecture.md`` lines 19512-19519.

    Attributes:
        original_path: Filesystem path to the original prompt file.
        new_prompt: The full text of the evolved prompt.
        version: Version string (e.g. "v2", "v3").
        changes_made: List of specific changes applied.
        knowledge_source: Research/knowledge that informed the evolution.
        evolved_at: UTC timestamp of the evolution.
    """

    original_path: str
    new_prompt: str
    version: str
    changes_made: List[str] = field(default_factory=list)
    knowledge_source: Dict[str, Any] = field(default_factory=dict)
    evolved_at: datetime = field(default_factory=utc_now)


# =============================================================================
# LEARNING
# =============================================================================


@dataclass
class Learning:
    """A piece of knowledge learned by the agent.

    Stored in the knowledge base (Supabase ``learnings`` table) and
    queried when making decisions about content strategy, prompt
    adjustments, and experiment design.

    Architecture reference: ``architecture.md`` lines 19641-19652.

    Attributes:
        id: Unique identifier (UUID string).
        topic: Topic or category of the learning.
        content: The actual knowledge content / description.
        source: How this learning was acquired (``"critique"``,
            ``"research"``, ``"experiment"``).
        confidence: Confidence score (0.0-1.0) in this learning.
        learned_at: UTC timestamp when the learning was created.
        applied_count: Number of times this learning has been applied.
        success_rate: Success rate when applied (``None`` if never applied).
    """

    id: str
    topic: str
    content: str
    source: str  # "critique", "research", "experiment"
    confidence: float
    learned_at: datetime = field(default_factory=utc_now)
    applied_count: int = 0
    success_rate: Optional[float] = None


# =============================================================================
# IMPROVEMENT RESULT
# Architecture reference: architecture.md lines 19767-19774
# =============================================================================


@dataclass
class ImprovementResult:
    """Result of a full deep improvement loop.

    Returned by the DeepImprovementLoop after running critique,
    reflection, knowledge extraction, and modifications.

    Attributes:
        original_draft: The original content draft that was evaluated.
        critique_summary: Summary of the critique dialogue.
        reflection: Reflection produced from the critique.
        knowledge_gained: Knowledge acquired during the loop.
        modifications: List of modifications applied or proposed.
        success: Whether the improvement loop completed successfully.
    """

    original_draft: str
    critique_summary: DialogueSummary
    reflection: Reflection
    knowledge_gained: List[Learning]
    modifications: List[ModificationRecord]
    success: bool


# =============================================================================
# EXPERIMENT VARIANT
# Architecture reference: architecture.md lines 19996-20008
# =============================================================================


@dataclass
class ExperimentVariant:
    """A single variant in an A/B experiment.

    Attributes:
        name: Variant name (``"control"`` or ``"treatment"``).
        description: What is different in this variant.
        config_override: Configuration overrides for this variant.
        posts: List of post IDs assigned to this variant.
        total_engagement: Sum of engagement metrics across all posts.
        avg_score: Average QC score across all posts in this variant.
    """

    name: str
    description: str
    config_override: Dict[str, Any]

    # Results
    posts: List[str] = field(default_factory=list)
    total_engagement: float = 0.0
    avg_score: float = 0.0


# =============================================================================
# EXPERIMENT
# Architecture reference: architecture.md lines 20010-20032
# =============================================================================


@dataclass
class Experiment:
    """A/B experiment definition and results.

    Tracks the full lifecycle of an experiment from design through
    completion and winner application.

    Attributes:
        id: Unique identifier for this experiment.
        name: Human-readable experiment name (e.g. "Question Hook vs Statement Hook").
        hypothesis: What the experiment is testing (e.g. "Question hooks
            will get 20% more engagement").
        variants: List of experiment variants.
        min_posts_per_variant: Minimum posts needed per variant before evaluation.
        max_posts_per_variant: Maximum posts per variant before forcing completion.
        status: Current lifecycle status.
        started_at: UTC timestamp when experiment began.
        completed_at: UTC timestamp when experiment finished.
        winner: Name of the winning variant, if determined.
        confidence: Statistical confidence in the winner (0.0-1.0).
        lift: Percentage improvement of winner over loser.
    """

    id: str
    name: str
    hypothesis: str

    # Design
    variants: List[ExperimentVariant] = field(default_factory=list)
    min_posts_per_variant: int = 5
    max_posts_per_variant: int = 10

    # Status
    status: ExperimentStatus = ExperimentStatus.DRAFT
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None

    # Results
    winner: Optional[str] = None
    confidence: float = 0.0
    lift: float = 0.0  # % improvement of winner over loser


# =============================================================================
# SINGLE-CALL EVALUATION MODELS
# Architecture reference: architecture.md lines 22937-23066
# =============================================================================


@dataclass
class EvaluationCriterion:
    """Single evaluation criterion with rubric.

    Used by ``SingleCallEvaluator`` to build the evaluation prompt.
    Each criterion has a weight (all weights MUST sum to 1.0), a rubric
    mapping scores to descriptions, and a guiding evaluation prompt.

    Attributes:
        name: Human-readable criterion name (e.g. "Hook Strength").
        weight: Weight for weighted total calculation (0.0-1.0).
        rubric: Mapping of score (int) to description (str).
        evaluation_prompt: Question for the evaluator to answer.
    """

    name: str
    weight: float
    rubric: Dict[int, str]  # score -> description
    evaluation_prompt: str


# The canonical evaluation rubric. Weights MUST sum to 1.0.
evaluation_rubric: Dict[str, EvaluationCriterion] = {
    "hook_strength": EvaluationCriterion(
        name="Hook Strength",
        weight=0.25,
        rubric={
            10: "Impossible to scroll past. Creates immediate curiosity gap or emotional reaction.",
            8: "Strong hook that stops most readers. Clear value proposition.",
            6: "Decent hook but generic. Could apply to many posts.",
            4: "Weak hook. Starts with background instead of punch.",
            2: "No hook. Buries the lede. Reader has no reason to continue.",
        },
        evaluation_prompt="Rate the first 2 lines. Do they stop the scroll?",
    ),
    "specificity": EvaluationCriterion(
        name="Specificity",
        weight=0.20,
        rubric={
            10: "Concrete numbers, named companies/tools, specific examples throughout.",
            8: "Good specificity with some concrete details.",
            6: "Mix of specific and vague. Some details, some generic statements.",
            4: "Mostly vague. Few concrete examples.",
            2: "Entirely abstract. No specific examples, numbers, or names.",
        },
        evaluation_prompt="Are there concrete numbers, names, and examples?",
    ),
    "value_density": EvaluationCriterion(
        name="Value Density",
        weight=0.20,
        rubric={
            10: "Every paragraph delivers actionable insight. No filler.",
            8: "High value throughout with minimal fluff.",
            6: "Some valuable insights mixed with filler.",
            4: "More padding than substance.",
            2: "No actionable takeaways. Pure fluff.",
        },
        evaluation_prompt="Does each paragraph add value? Are there actionable takeaways?",
    ),
    "authenticity": EvaluationCriterion(
        name="Authenticity",
        weight=0.15,
        rubric={
            10: "Unmistakably human. Unique voice, personal touches, natural flow.",
            8: "Sounds human with minor AI tells.",
            6: "Could be human or AI. Generic but not obviously artificial.",
            4: "Some AI patterns visible. Overly formal or structured.",
            2: "Obviously AI-generated. Robotic, formulaic, or uses AI phrases.",
        },
        evaluation_prompt="Does this sound like a real person wrote it?",
    ),
    "structure": EvaluationCriterion(
        name="Structure",
        weight=0.10,
        rubric={
            10: "Perfect scannability. White space, short paragraphs, clear flow.",
            8: "Good structure, easy to read.",
            6: "Acceptable structure but could be improved.",
            4: "Hard to scan. Dense paragraphs.",
            2: "Wall of text. No visual breaks.",
        },
        evaluation_prompt="Is it easy to scan? Good use of white space?",
    ),
    "cta_clarity": EvaluationCriterion(
        name="CTA Clarity",
        weight=0.10,
        rubric={
            10: "Clear, natural call-to-action that invites engagement.",
            8: "Good CTA, clear next step for reader.",
            6: "CTA present but weak or generic.",
            4: "Unclear what reader should do.",
            2: "No CTA or abrupt ending.",
        },
        evaluation_prompt="Is there a clear next step for the reader?",
    ),
}


@dataclass
class SingleCallEvaluation:
    """Complete evaluation from a single LLM call.

    Architecture reference: ``architecture.md`` lines 23044-23066.

    Attributes:
        scores: Mapping of criterion name to integer score (1-10).
        weighted_total: Weighted average of all criterion scores.
        criterion_feedback: Per-criterion feedback with quote, score, explanation.
        strengths: What is working well (2-3 items).
        weaknesses: What needs improvement (2-3 items).
        specific_suggestions: Actionable improvements (3-5 items).
        passes_threshold: Whether the weighted total meets the threshold.
        recommended_revisions: Prioritized revisions if threshold not met.
        patterns_detected: Recurring issues across posts.
        knowledge_gaps: What the Writer should research.
    """

    # Scores
    scores: Dict[str, int]  # criterion_name -> score (1-10)
    weighted_total: float

    # Detailed feedback for each criterion
    criterion_feedback: Dict[str, dict]  # criterion -> {quote, score, explanation}

    # Summary feedback
    strengths: List[str]  # What's working well (2-3 items)
    weaknesses: List[str]  # What needs improvement (2-3 items)
    specific_suggestions: List[str]  # Actionable improvements (3-5 items)

    # Decision
    passes_threshold: bool  # weighted_total >= threshold
    recommended_revisions: Optional[List[str]]  # If doesn't pass

    # For learning
    patterns_detected: List[str]  # Recurring issues across posts
    knowledge_gaps: List[str]  # What Writer should research


@dataclass
class VisualEvaluation:
    """Result of visual quality evaluation.

    Architecture reference: ``architecture.md`` lines 23307-23316.

    Attributes:
        score: Overall visual quality score (1-10).
        format_appropriate: Whether the visual format suits the content type.
        content_match_score: How well the visual matches the post content (1-10).
        technical_quality: Technical quality rating (resolution, contrast, etc.).
        brand_consistency: Whether the visual matches brand guidelines.
        issues: List of problems to fix.
        strengths: List of what is working well.
    """

    score: float  # 1-10
    format_appropriate: bool
    content_match_score: float  # How well visual matches post content
    technical_quality: float  # Resolution, contrast, readability
    brand_consistency: bool
    issues: List[str]  # Problems to fix
    strengths: List[str]  # What's working


# =============================================================================
# MODIFICATION SAFETY MODELS
# Architecture reference: architecture.md lines 22438-22895
# =============================================================================


# Canonical risk classification for all known modification types.
modification_risk_classification: Dict[str, ModificationRiskLevel] = {
    # LOW -- auto-apply, monitor for 5 posts
    "posting_time_adjustment": ModificationRiskLevel.LOW,
    "emoji_count_change": ModificationRiskLevel.LOW,
    "hashtag_count_change": ModificationRiskLevel.LOW,
    # MEDIUM -- auto-apply with automatic rollback trigger
    "template_weight_adjustment": ModificationRiskLevel.MEDIUM,
    "hook_style_preference": ModificationRiskLevel.MEDIUM,
    "visual_format_preference": ModificationRiskLevel.MEDIUM,
    "content_type_distribution": ModificationRiskLevel.MEDIUM,
    # HIGH -- requires human approval before apply
    "writer_prompt_change": ModificationRiskLevel.HIGH,
    "qc_criteria_change": ModificationRiskLevel.HIGH,
    "new_hook_template": ModificationRiskLevel.HIGH,
    # CRITICAL -- requires human approval + explicit confirmation
    "new_code_module": ModificationRiskLevel.CRITICAL,
    "scoring_algorithm_change": ModificationRiskLevel.CRITICAL,
    "core_pipeline_change": ModificationRiskLevel.CRITICAL,
}


@dataclass
class ModificationRequest:
    """Request to modify system behavior.

    Tracks the full lifecycle of a modification: from initial request
    through approval/rejection to potential rollback.

    Architecture reference: ``architecture.md`` lines 22486-22516.

    Attributes:
        id: Unique identifier for this modification request.
        modification_type: Type key from ``modification_risk_classification``.
        risk_level: Risk level determining the approval flow.
        component: Target component (e.g. "writer", "qc").
        before_state: Configuration state before the change.
        after_state: Configuration state after the change.
        reasoning: Human-readable explanation of why the change is needed.
        triggered_by: What triggered this modification.
        supporting_data: Evidence supporting the change.
        status: Current status in the lifecycle.
        human_approver: Who approved (if human-approved).
        approved_at: When it was approved.
        created_at: When the request was created.
        expires_at: When pending approval expires.
        reminder_at: When to send next reminder.
        rejected_reason: Why it was rejected (if rejected).
        modification_chain_id: Links related modifications together.
    """

    id: str
    modification_type: str
    risk_level: ModificationRiskLevel

    # What's changing
    component: str  # "writer", "trend_scout", "qc", etc.
    before_state: dict
    after_state: dict

    # Why
    reasoning: str
    triggered_by: str  # "research", "analytics", "critique"
    supporting_data: dict  # Evidence for the change

    # Approval tracking
    status: str  # "pending", "approved", "rejected", "auto_applied", "rolled_back"
    human_approver: Optional[str] = None
    approved_at: Optional[datetime] = None

    # Audit and timeout fields
    created_at: datetime = field(default_factory=utc_now)
    expires_at: Optional[datetime] = None
    reminder_at: Optional[datetime] = None
    rejected_reason: Optional[str] = None
    modification_chain_id: Optional[str] = None


@dataclass
class RollbackTrigger:
    """Conditions that trigger automatic rollback of a modification.

    Architecture reference: ``architecture.md`` lines 22519-22526.

    Attributes:
        metric: Which metric to monitor (e.g. "engagement_rate").
        threshold: Minimum acceptable ratio of new to baseline performance.
        window_posts: Number of posts to evaluate before deciding.
        baseline_value: Metric value before the modification was applied.
    """

    metric: str  # "engagement_rate", "qc_pass_rate", "human_approval_rate"
    threshold: float  # e.g., 0.8 (20% drop from baseline)
    window_posts: int  # Number of posts to evaluate
    baseline_value: float  # Value before modification


# =============================================================================
# APPROVAL TIMEOUT CONFIG
# Architecture reference: architecture.md lines 20956-20993
# =============================================================================


@dataclass
class ApprovalTimeoutConfig:
    """Configuration for approval timeout handling.

    Controls reminder intervals, escalation thresholds, and
    auto-resolution behavior for pending human approvals.

    Attributes:
        reminder_after_hours: Hours before first reminder is sent.
        reminder_interval_hours: Hours between follow-up reminders.
        max_reminders: Maximum number of reminders before escalation.
        escalate_after_hours: Hours before escalating to additional contacts.
        escalation_contacts: Additional contacts for escalation.
        auto_resolve_after_hours: Hours before auto-resolution.
        auto_resolve_action: Action on auto-resolution
            (``"approve"``, ``"reject"``, or ``"hold"``).
        auto_resolve_min_score: Only auto-approve if QC score >= this.
    """

    # Reminder settings
    reminder_after_hours: int = 4
    reminder_interval_hours: int = 4
    max_reminders: int = 3

    # Escalation settings
    escalate_after_hours: int = 24
    escalation_contacts: Optional[List[str]] = None

    # Auto-resolution settings
    auto_resolve_after_hours: int = 48
    auto_resolve_action: str = "approve"  # "approve", "reject", or "hold"
    auto_resolve_min_score: float = 8.0

    def __post_init__(self) -> None:
        """Comprehensive validation of configuration values."""
        if self.escalation_contacts is None:
            self.escalation_contacts = []

        valid_actions = {"approve", "reject", "hold"}
        if self.auto_resolve_action not in valid_actions:
            raise ValueError(
                f"auto_resolve_action must be one of {valid_actions}, "
                f"got '{self.auto_resolve_action}'"
            )

        if not (
            self.reminder_after_hours
            < self.escalate_after_hours
            < self.auto_resolve_after_hours
        ):
            raise ValueError(
                "Must have: reminder_after_hours < escalate_after_hours "
                "< auto_resolve_after_hours"
            )

        if not 0.0 <= self.auto_resolve_min_score <= 10.0:
            raise ValueError(
                f"auto_resolve_min_score must be 0-10, "
                f"got {self.auto_resolve_min_score}"
            )


# =============================================================================
# PENDING APPROVAL
# Architecture reference: architecture.md lines 20996-21007
# =============================================================================


@dataclass
class PendingApproval:
    """Represents a pending approval request.

    Tracked by the ApprovalTimeoutManager to handle reminders,
    escalation, and auto-resolution.

    Attributes:
        run_id: Unique identifier for the pipeline run.
        post_id: ID of the post awaiting approval, if applicable.
        requested_at: UTC timestamp when approval was requested.
        content_type: Content type of the post.
        qc_score: QC score of the post.
        reminder_count: Number of reminders sent so far.
        escalation_level: Current escalation level (0=pending, 1=reminded,
            2=escalated, 3=auto-resolved).
        notification_channel: Channel used for notifications
            (``"telegram"``, ``"email"``, ``"slack"``).
    """

    run_id: str
    post_id: Optional[str]
    requested_at: datetime
    content_type: str
    qc_score: float
    reminder_count: int = 0
    escalation_level: int = 0  # 0=pending, 1=reminded, 2=escalated, 3=auto-resolved
    notification_channel: str = "telegram"


# =============================================================================
# AUTONOMY CONFIG
# Architecture reference: architecture.md lines 20638-20665
# =============================================================================


@dataclass
class AutonomyConfig:
    """Configuration for autonomy management.

    Controls default autonomy level, per-content-type overrides,
    auto-publish threshold, and auto-degradation behavior after
    consecutive failures.

    Attributes:
        default_level: Default autonomy level for all content types.
        auto_publish_threshold: Minimum QC score for auto-publishing
            at ``AUTO_HIGH_SCORE`` level (0.0-10.0).
        content_type_levels: Per-content-type autonomy level overrides.
        auto_degradation_enabled: Whether to auto-degrade after failures.
        consecutive_failures_threshold: Number of consecutive failures
            before triggering auto-degradation (must be >= 1).
        degradation_duration_hours: How long the degraded level persists
            (must be > 0).
    """

    default_level: AutonomyLevel = AutonomyLevel.AUTO_HIGH_SCORE
    auto_publish_threshold: float = 9.0

    # Per content-type level overrides
    content_type_levels: Dict[str, AutonomyLevel] = field(default_factory=dict)

    # Auto-degradation after consecutive failures
    auto_degradation_enabled: bool = True
    consecutive_failures_threshold: int = 3
    degradation_duration_hours: int = 24

    def __post_init__(self) -> None:
        """Validate configuration values."""
        if not 0.0 <= self.auto_publish_threshold <= 10.0:
            raise ValueError(
                f"auto_publish_threshold must be 0-10, "
                f"got {self.auto_publish_threshold}"
            )
        if self.consecutive_failures_threshold < 1:
            raise ValueError(
                f"consecutive_failures_threshold must be >= 1, "
                f"got {self.consecutive_failures_threshold}"
            )
        if self.degradation_duration_hours <= 0:
            raise ValueError(
                f"degradation_duration_hours must be > 0, "
                f"got {self.degradation_duration_hours}"
            )


# =============================================================================
# CONFIG PATH MAPPING (single source of truth)
# Architecture reference: architecture.md lines 22867-22895
# =============================================================================

CONFIG_PATH_MAPPING: Dict[str, str] = {
    "writer": "config/writer_config.json",
    "trend_scout": "config/trend_scout_config.json",
    "analyzer": "config/analyzer_config.json",
    "humanizer": "config/humanizer_config.json",
    "visual_creator": "config/visual_creator_config.json",
    "qc": "config/qc_config.json",
    "scheduler": "config/scheduler_config.json",
    "meta_agent": "config/meta_agent_config.json",
}


def get_config_path(component: str) -> str:
    """Get config file path for a component. Single source of truth.

    Uses whitelist-only approach for security (prevents path traversal).

    Args:
        component: Component name (e.g., ``"writer"``, ``"qc"``).

    Returns:
        Relative path to the component's configuration file.

    Raises:
        ValueError: If the component is not recognized.
    """
    if component not in CONFIG_PATH_MAPPING:
        raise ValueError(
            f"Unknown component: {component}. "
            f"Valid components: {list(CONFIG_PATH_MAPPING.keys())}"
        )
    return CONFIG_PATH_MAPPING[component]


# =============================================================================
# PUBLIC API
# =============================================================================

__all__ = [
    # Enums
    "CapabilityType",
    "ResearchTrigger",
    "ModificationRiskLevel",
    "ExperimentStatus",
    "AutonomyLevel",
    # Capability gap detection & code generation
    "CapabilityGap",
    "GeneratedCode",
    "RegisteredModule",
    "SelfModificationResult",
    # Claude Code CLI
    "ClaudeCodeResult",
    # Research models
    "ResearchQuery",
    "ResearchFinding",
    "ResearchRecommendation",
    "ResearchReport",
    # Rollback & snapshots
    "SystemSnapshot",
    "RollbackResult",
    # Dialogue & reflection
    "DialogueSummary",
    "Reflection",
    # Self-modification records
    "ModificationRecord",
    # Code evolution
    "GeneratedModule",
    "PromptEvolution",
    # Knowledge
    "Learning",
    # Improvement loop
    "ImprovementResult",
    # Experimentation
    "ExperimentVariant",
    "Experiment",
    # Evaluation models
    "EvaluationCriterion",
    "evaluation_rubric",
    "SingleCallEvaluation",
    "VisualEvaluation",
    # Modification safety models
    "modification_risk_classification",
    "ModificationRequest",
    "RollbackTrigger",
    # Approval & autonomy
    "ApprovalTimeoutConfig",
    "PendingApproval",
    "AutonomyConfig",
    # Config path
    "CONFIG_PATH_MAPPING",
    "get_config_path",
]
